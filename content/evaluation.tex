\chapter{Evaluating carbon-aware scheduling with the new job model} \label{sec:evaluate_scheduling}

In order to evaluate the carbon savings made possible by the previously established dynamic power model, the following approach is proposed:
As the already existing job traces hold no information regarding the power or their phases, some test cases will be presented and used for the models.
In a rather explorative process, we use the following parameters and run the simulation on all possible permutations of them. 
This is done using the \verb|generate_evaluation_jobs.sh| script, but a high-level view of the used parameters is given thus:

\begin{enumerate}
    \item Checkpoint \& restore scheduling or non-interrupted scheduling
    \item Different phase configurations, in general, we assume there to be a (long or short) high-powered phase and a (long or short) low-powered phase periodically up to the given job length. 
    \item Startup-lengths between none and 5 minutes
    \item Startup power levels between 100 W and 300 W
    \item Different deadlines picked from 4 hours, 12 hours, and 1 day
    \item Different Job lengths, from 1 hour to 3 hours. \label{eval_list:lengths}
\end{enumerate}

Another dimension will be comparing these scenarios against having no information on phases, instead only having an averaged constant wattage of the otherwise existing phases.
This would represent the previous \verb|GAIA| implementation, where jobs had a constant amount of power at all points in time. 

\section{Running the Evaluation on a Cluster}

As previously discussed in Section \ref{sec:checkpoint_resume_lp}, calculating the checkpoint \& resume scheduling tends to have high runtime and memory requirements. 
For that reason, the simulation was executed on the \emph{SCORE Lab}, standing for "scientific compute resources", of HPI.

After requesting and gaining access, \verb|rsync| was used to transfer the simulation to the cluster. 
Slurm could then be used to schedule all experiments individually (that were generated using the mentioned \verb|generate_all_jobs.sh| script), which would parallelize the evaluation, leading to faster results.

One problem arose in the licensing of the used \emph{Gurobi} solver. 
Under the used academic license, only 2 \emph{"sessions"} are allowed simultaneously.
A session is defined via the hostname of the executing machine, which is communicated to the solver's licensing servers live.
As such, scheduling each experiment by itself lead to many sessions being started, as Slurm would distribute the jobs on the available nodes.
While there is some leeway in the amount of sessions, experiments would crash beyond the 5th instance as the solver would not start.

We work under this restriction by distributing the experiments across two workers (via a script called \verb|evalute_dynamic_power.sh|); as each experiment has an index, one worker executes the even numbered jobs and the other executes the odd ones. 
Using the even-odd strategy results in both workers having about the same amount of work.

The workers in this case are python docker containers that were created with the help SCORE Lab's online knowledge base.
The command for launching one example worker, in this case the one for even jobs, is given in Listing \ref{list:evaluation_slurm}.

\begin{lstlisting}[language=bash, frame=single, numbers=left, caption={Executing the Evaluation inside the SCORE Lab's Slurm environment}, label={list:evaluation_slurm}, basicstyle=\ttfamily]
sbatch -A polze -p magic \
    --container-image=python \
    --container-name=test \
    --container-writable \
    --mem=128G \
    --cpus-per-task=128 \
    --time=24:0:0 \
    --output=slurmlogs/output_%j.txt \
    --error=slurmlogs/error_%j.txt \
    --container-mounts=/hpi/fs00/home/vincent.opitz:/home/vincent.opitz \
    --container-workdir=/home/vincent.opitz/master-thesis/GAIA \
    run_all_even_jobs.sh
 \end{lstlisting}

Now the simulation can be run with 128 GB RAM as specified in line 5. 
The \verb|-p magic| parameter results in us using a compute node. 
Line 3, \verb|--container-writable|, means that the container will be reused if called again under the same name, this lets us conduct set up using (\verb|pip install|).

\section{Results for uninterrupted scheduling}

\section{Results for checkpoint \& resume scheduling}
